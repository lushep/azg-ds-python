{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Advanced\n",
    "\n",
    "### In this notebook we will review and practice with other scikit-learn features\n",
    "\n",
    "- Preprocessing   \n",
    "- Creating and fitting Pipelines  \n",
    "- Cross-Validation \n",
    "- Grid Search\n",
    "- and more...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "In this notebook we will work with the [Titanic dataset](https://www.kaggle.com/c/titanic/data), containing historical data about Titanic passengers and whether they survived the wreck.\n",
    "\n",
    "<img src=\"images/04_sklearn_advanced/titanic.png\" style=\"display: block;margin-left: auto;margin-right: auto;width: 600px\"/>\n",
    "\n",
    "We will first focus on sklearn's preprocessing features, but before we can go ahead, let's load the dataset and get it ready for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titanic_df = sns.load_dataset('titanic')\n",
    "print(titanic_df.shape)\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial analysis\n",
    "\n",
    "Clearly some issues need to be resolved before we can do machine learning on this dataset.\n",
    "\n",
    "1) Take some time to examine the dataset. Below are some suggestions for what you may want to investigate. Note that this is not an exhaustive list; see if you can find something interesting!\n",
    "\n",
    "    - How much data do we have? how many features?\n",
    "    - What do the features represent\n",
    "    - What datatypes does it contain? Are there any missing values?\n",
    "    \n",
    "    - Investigate how many different values some of the categorical features contain\n",
    "    - Is there any redundant information?\n",
    "    \n",
    "    - Produce some summary statistics for the different features\n",
    "    - Are any of the features correlated?\n",
    "    - Group the data by the survived column and compare statistics\n",
    "    \n",
    "2) Which ML models would you consider for this problem? Why have you made this choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tool for some automatic analysis of a pandas DataFrame\n",
    "# from pandas_profiling import ProfileReport\n",
    "# profile = ProfileReport(titanic_df, minimal=True)\n",
    "# profile.to_file('Titanic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our feature matrix and target vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:\n",
    "\n",
    "Raw datasets are often not suitable for machine learning algorithms. For example, the dataset may contain categorical features or have missing values. Preprocessing the dataset to ensure that machine learning is feasible is therefore an import phase of a project.\n",
    "\n",
    "In the dataset we found plenty of interesting variables, though quite a few need preprocessing. Let's first remove the redundant columns and drop rows with missing values*.\n",
    "\n",
    "**With Scikit-Learn we can actually impute missing values but for now this approach will be sufficient*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove irrelevant features & missing values.\n",
    "# Later we will discuss alternatives to this.\n",
    "titanic_processed = (\n",
    "    titanic_df\n",
    "    .drop(['embarked', 'sex', 'adult_male','deck','alive', 'class'], axis=1)\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Get the feature matrix & target vector\n",
    "titanic_features = (\n",
    "    titanic_processed\n",
    "    .drop(['survived'], axis=1)\n",
    ")\n",
    "titanic_labels = titanic_processed['survived']\n",
    "\n",
    "# Notice that we now use Pandas dataframe & series rather than numpy arrays - this is also possible with sklearn!\n",
    "print(titanic_features.shape, titanic_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable encoding\n",
    "\n",
    "It would still be rather difficult to work with the raw dataset. One reason is that it contains categorical values, e.g passenger class comes as text: \"First\", \"Second\" and \"Third\". However, it is possible to encode these categorical variables into numeric ones. Fortuantely, scikit-learn makes it easy for us to do so.\n",
    "\n",
    "### Encoding the target vector\n",
    "\n",
    "We can numerically encode a column using the `LabelEncoder` from Scikit-Learn.\n",
    "\n",
    "Normally this is used for the target or label column (although in our case this is not necessary). However, it can also be used for columns in the feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like the machine learning estimators, in Scikit-Learn, the preprocessing algorithms are implemented as Python objects. They are referred to as *transformers*.\n",
    "\n",
    "Once you have picked the transformer algorithm you will use, you instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers have a `.fit()` method implemented so that they can learn the paramters to transform data. The `.transform()` method is then used to perform the transformation. Let's demonstrate this on the alone column of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before transformation:')\n",
    "print(titanic_features['alone'])\n",
    "\n",
    "label_encoder.fit(titanic_features['alone'])\n",
    "new_col = label_encoder.transform(titanic_features['alone'])\n",
    "\n",
    "print('\\n\\nAfter transformation:')\n",
    "print(new_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They also have a `.fit_transform()` method implemented so that the transformation can be performed directly after the paramters have been learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before transformation:')\n",
    "print(titanic_features['alone'])\n",
    "\n",
    "new_col = label_encoder.fit_transform(titanic_features['alone'])\n",
    "\n",
    "print('\\n\\nAfter transformation:')\n",
    "print(new_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinally encoding features\n",
    "\n",
    "For the feature matrix it would be more practical to encode all categorical features at once (rather than one column at a time). \n",
    "\n",
    "One way of doing this could be to use the `OrdinalEncoder`, which can ordinally encode multiple features at the same time.\n",
    "\n",
    "However, we do not want to encode *all* of the features, only those that are categorical. Rather than doing this manually, we can use `ColumnTransformer` to achieve exactly that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "categorical_columns = [ 'who', 'embark_town', 'alone']\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "    (\"ordinal\", OrdinalEncoder(), categorical_columns)\n",
    "    ], remainder=\"passthrough\") \n",
    "\n",
    "# The output of fit_transform is no longer a pandas df, but now a numpy matrix. \n",
    "feature_matrix = ct.fit_transform(titanic_features)\n",
    "\n",
    "print(feature_matrix[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the ColumnTransformer will affect the order of your columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding features\n",
    "\n",
    "What issues could we get from using an `OrdinalEncoder()` on some of the categorical values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way of encoding categorial values may work for problems where there is a natural ordinal relationship between the categories, and in turn the integer values, such as labels for temperature ‘cold’, warm’, and ‘hot’.\n",
    "\n",
    "There may be problems when there is no ordinal relationship and allowing the representation to lean on any such relationship might be damaging to learning to solve the problem. An example might be the labels ‘red’, 'blue' and ‘green’\n",
    "\n",
    "In these cases, we would like to give the algorithm more expressive power, we require [one-hot encoding](https://gdd.li/04_sklearn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we can use sklearn's `OneHotEncoder()` for this purpose. This can also help us encode multiple features at the same time.\n",
    "\n",
    "Again, we do not want to encode *all* of the features. We only want to pass some of the columns into the `OneHotEncoder()`. Like before, we use the `ColumnTransformer()` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(titanic_features.shape)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "    (\"onehot\", OneHotEncoder(drop='first'), [ 'who', 'embark_town', 'alone'])\n",
    "    ], remainder=\"passthrough\") \n",
    "\n",
    "# The output of fit_transform is no longer a pandas df, but now a numpy matrix. \n",
    "feature_matrix = ct.fit_transform(titanic_features)\n",
    "\n",
    "print(feature_matrix[0:5])\n",
    "\n",
    "print(feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keep in mind:**\n",
    "\n",
    "We use the `drop='first'` option in OneHotEncoder, which ensures that we keep *n-1* dummies per categorical variable.\n",
    "\n",
    "`ColumnTransformer` can also simultaneously apply a different transformation to other columns if we specify an extra step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "\n",
    "Your preprocessed data may contain attributes with a mixtures of scales for various quantities such as dollars, kilograms and sales volume. \n",
    "\n",
    "However, many machine learning algorithms perform better when numerical input variables are scaled to a standard range. [For example](https://gdd.li/04_sklearn), algorithms that use distance measures, like k-nearest neighbors.\n",
    "\n",
    "We want to ensure our variables share a similar scale. However, it is important to be aware of when to do feature scaling to avoid data leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Data leakage warning!!!*\n",
    "\n",
    "We want to avoid any information from the test set leaking into the training set; it is important our ML algorithm learns only from the training data! \n",
    "\n",
    "If information from the test set *does* leak into the training data, it may cause our metrics to overestimate our model's performance.\n",
    "\n",
    "Therefore we must do the train-test split *before* we do feature scaling (and other forms of preprocessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename to X and y for consistency. \n",
    "X = feature_matrix\n",
    "y = titanic_labels\n",
    "\n",
    "print(type(X))\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=111, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing classification, we can set `stratify` to ensure a consistent split across training and test sets. This can be useful to get a more reliable score for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling with sklearn\n",
    "\n",
    "`StandardScaler()` and `MinMaxScaler()` from `sklearn.preprocessing` allow us to scale the data together with applying the `fit_transform()` method\n",
    "\n",
    "Remember to always first separate the data into train/test (and validation) sets and only then use feature scaling. If you take the mean and variance of the whole dataset you'll be introducing future information into the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `StandardScaler()` transforms features by removing the mean and scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_X = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After instantiating the transformer, we learn *how* to transform the dataset on the training set with `.fit()`\n",
    "\n",
    "We then transfom the train set with `.transform()`.\n",
    "\n",
    "These two steps can be performed at once with `.fit_transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = sc_X.fit_transform(X_train)\n",
    "X_train_scaled.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the `.transform()` on the test set,  we ensure it is encoded in the same way as the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MinMaxScaler()` transforms features to be with a certain range, e.g. 0 & 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_X = MinMaxScaler()\n",
    "X_train_scaled = sc_X.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = sc_X.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other scaling options to use too. For example, the `RobustScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "help(RobustScaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing with Scikit-Learn\n",
    "\n",
    "Further preprocessing tools from `sklearn.preprocessing` could also be applied to our dataset. For example,\n",
    "- Binarization: `Binarizer()`\n",
    "\n",
    "    A common operation on text count data where the analyst can decide to only consider the presence or absence of a feature rather than a quantified number of occurrences for instance.\n",
    "    \n",
    "\n",
    "- Imputing Missing Values: `Imputer()`\n",
    "\n",
    "    Address missing values in a column by imputting the mean, median, mode or another constant value.\n",
    "    \n",
    "\n",
    "- Generating Polynomial Features: `PolynomialFeatures()`\n",
    "\n",
    "    Generate a new feature matrix consisting of all polynomial combinations of the features. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]. This can help the model to learn more complex relationships.\n",
    "    \n",
    "\n",
    "### Scikit-Learn transformers\n",
    "\n",
    "The diagrams below are visualisations of a Scikit-Learn transformer.\n",
    "\n",
    "`.fit()`\n",
    "\n",
    "<img src=\"images/04_sklearn_advanced/transform.png\" style=\"display: block;margin-left: auto;margin-right: auto;width: 600px\"/>\n",
    "\n",
    "`.fit_transform()`\n",
    "\n",
    "<img src=\"images/04_sklearn_advanced/fit_transform.png\" style=\"display: block;margin-left: auto;margin-right: auto;width: 600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "> **\"If you aren’t using pipelines you’re probably doing [Scikit-Learn] wrong.\"** - [Andreas Muller, Core Developer of Scikit-learn ](https://towardsdatascience.com/want-to-truly-master-scikit-learn-2-essential-tips-from-the-official-developer-himself-dada6ff56b99)\n",
    "\n",
    "<img src=\"images/04_sklearn_advanced/sklearn-pipe.png\" style=\"display: block;margin-left: auto;margin-right: auto;width: 600px\"/>\n",
    "\n",
    "#### What is a pipeline?\n",
    "\n",
    "Whilst the statement above was probably an exaggeration, they are a great way to keep your code clean, consistent and mistake-free. \n",
    "\n",
    "Pipelines encapsulate all the preprocessing steps (feature selections, scaling, encoding of variables and so on), as well as the final model, into a single Scikit-Learn estimator. \n",
    "\n",
    "- Pipelines simplify and automate many steps in preprocessing and model training. \n",
    "- They give your workflow order and make it easier to read and understand. Later we will see how they can also be very useful during model optimization. \n",
    "- In addition to this including preprocessing as part of our model pipeline we can **avoid information leaks**\n",
    "\n",
    "#### Getting started with a simple pipeline for preprocessing\n",
    "\n",
    "An sklearn `Pipeline` simply requires us to specify a number of steps and what should happen at each of them. In our case we are going to add interaction terms for our features using `PolynomialFeatures()` and then scale the resulting data with `MinMaxScaler()` (we are not going to use `X_train_scaled` from the previous step).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "preprocess_pipeline = Pipeline(steps=[\n",
    "    ('PFeatures', PolynomialFeatures(interaction_only=True)),\n",
    "    ('scaler', MinMaxScaler())])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple ML pipeline\n",
    "\n",
    "We can further extend this pipeline by adding the last step containing our machine learning model.\n",
    "\n",
    "We shall demonstrate this by adding an ML model to our pipeline (an SVM classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "titanic_pipeline = Pipeline(steps=[\n",
    "    ('PFeatures', PolynomialFeatures(2)),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('model', SVC(kernel='linear'))])\n",
    "\n",
    "titanic_pipeline.fit(X_train, y_train)\n",
    "y_pred = titanic_pipeline.predict(X_test)\n",
    "print(\"accuracy: \", round(accuracy_score(y_test, y_pred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beauty of sklearn pipelines is in the fact that it will also learn how to scale the data based on the `X_train` features and then automatically use it when we get predictions from `X_test`. So in practice it also removes some preprocessing efforts and unnecessary risks away from us.\n",
    "\n",
    "The diagram below is a visualisation of Scikit-Learn Pipeline\n",
    "\n",
    "<img src=\"images/04_sklearn_advanced/pipe-transform.png\" style=\"display: block;margin-left: auto;margin-right: auto;width: 600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a pipeline has been specified, model training is not much different from training a regular model. \n",
    "\n",
    "This also means we can easily evaluate whether certain preprocessing settings help our model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "When training our models, we should reserve a separate set of data for comparing model performance and picking hyperparameters, i.e. Should I use order 2 polynomial features or order 3?\n",
    "\n",
    "The test set should only be used to give a final evaluation of the model.\n",
    "\n",
    "\n",
    " <img src=\"images/04_sklearn_advanced/validation.png\" style=\"display: block;margin-left: auto;margin-right: auto;height: 100px\"/>\n",
    " \n",
    "Otherwise information from the test set leaks into our training process invalidating our metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "If we only have a small dataset, we may not have enough data to create a seperate training set. In addition to this, our validation results will still be biased to that split of the train/validation set. Cross-validation is used to address these issues.\n",
    "\n",
    " <img src=\"images/04_sklearn_advanced/crossvalidation.png\" style=\"display: block;margin-left: auto;margin-right: auto;height: 300px\"/>\n",
    "\n",
    "K-fold cross validation splits can be acquired using `KFold()` from `sklearn.model_selection`. Then, for each split, the model can be fit and relevant metrics can be computed.\n",
    "\n",
    "It may be however more practical to use `cross_val_score()` from `sklearn.model_selection`. It returns an array of cross-validated scores for a model/pipeline on the data.\n",
    "\n",
    "Let's see how our current model performs on the different validation splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(titanic_pipeline,\n",
    "                         X_train,\n",
    "                         y_train,\n",
    "                         cv = 5,\n",
    "                         scoring = 'accuracy'\n",
    "                        )\n",
    "\n",
    "print(f'Accuracy per fold: {scores.round(3)}')\n",
    "print(f'Accuracy mean over folds: {scores.mean().round(3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning\n",
    "\n",
    "To find the best paramters, we often want to check how each combination of paramters performs. This process in known as a grid seach.\n",
    "\n",
    "We could manually collect the cross validation scores for each paramter combination. However, in Scikit-Learn this can be implemented directly.\n",
    "\n",
    "### Grid Search\n",
    "\n",
    "`GridSearchCV()` from `sklearn.model_selection` allows tuning of model's parameters using cross-validated grid search. \n",
    "\n",
    "It requires a dictionary of parameters and the options it should compare. This does not only have to be model parameters, but parts of preprocessing too. We simply need to reference the correct pipeline step using 'step_name__parameter_name' as one of the dictionary keys (note the double underscore between the step and parameter name). In the end, the best set of parameters will be selected based on the provided metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to the way the params dictionary is written:\n",
    "- A key is a string of the name of the model + 2 underscores + the specific parameter to search, for example classifier__criterion\n",
    "- A value is a list/tuple of all the parameters values you want to search\n",
    "\n",
    "This allows us to search possible parameters for any of the steps in the initial Pipeline (e.g. preprocessing too!) \n",
    "\n",
    "Note that with large datasets, many parameters and many cross validation folds, `GridSearchCV()` may take considerable time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#providing the list of parameters\n",
    "parameters_svm = {'PFeatures__degree': (1,2),\n",
    "                 'scaler': [MinMaxScaler(),StandardScaler(),RobustScaler() ]}\n",
    "\n",
    "#running grid search \n",
    "clf_svm = GridSearchCV(titanic_pipeline, parameters_svm, cv=5, scoring='accuracy')\n",
    "clf_svm = clf_svm.fit(X_train, y_train)\n",
    "\n",
    "print(f'CV accuracy score of the best SVM is: {clf_svm.best_score_:.3f}')\n",
    "print(f'Best parameters were: {clf_svm.best_params_}')\n",
    "#parameter importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to being easier to implement than a manual grid search cross-validation, `GridSearchCV()` helps us to avoid __information leaks__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Once we've established the best parameters, we should use them to retrain on the *WHOLE* training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refitting on the whole training set.\n",
    "#clf_svm.set_params(refit=True)\n",
    "#clf_svm.fit(X_train, y_train)\n",
    "best_model=clf_svm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy on the test set: {best_model.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is however usually a good idea to not just blindly take the \"best\" model, but manually compare different models from the Grid search. Higher accuracy often comes at the price of over-fitting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(clf_svm.cv_results_)\n",
    "cv_results.sort_values('rank_test_score').head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have finished, it will likely be helpful to save you model for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import dump, load\n",
    "# # Save the model\n",
    "# dump(best_model, 'best_model.joblib') \n",
    "\n",
    "# Load a saved model\n",
    "#best_model = load('best_model.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "1. Create a new ML pipeline with (somewhat) different preprocessing steps and a different ML model\n",
    "2. Get the CV_f1 score (or accuracy) for that pipeline\n",
    "3. Play around with Grid search (change some other parameters in the param_dict) \n",
    "\n",
    "Can you create a model better than this current one? What were your reasons for picking the ML model you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Scikit-Learn can be a very powerful tool to deal with machine learning problems including data splitting, preprocessing, model training and model selection. Its simple interface and detailed documentation allow it to be used even by users with little experience.\n",
    "\n",
    "* A huge variety of implemented models with sensible defaults, such as KMeans, SVC or RandomForest\n",
    "* Supports both pandas and numpy as data inputs\n",
    "* Data preprocessing techniques such as Scaler, OneHotEncoding, etc. \n",
    "* Validation tools such as train_test_split or cross validation\n",
    "* Pipelines to evaluate not only your model and its hyperparameters, but also your preprocessing steps.\n",
    "* GridSearch for parameter tuning selection\n",
    "* Consistent implementation and API which makes it easy to extend with your own implemented building blocks (scorers, preprocessing techniques and even models)\n",
    "\n",
    "Caveat: scikit-learn lacks tools to work with deep learning. For this purpose you would rather use deep learning libraries such as PyTorch or Tensorflow/Keras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
