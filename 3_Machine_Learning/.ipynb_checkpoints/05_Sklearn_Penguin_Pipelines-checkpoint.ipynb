{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines: Palmer Penguins continued\n",
    "\n",
    "![](https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/lter_penguins.png) \n",
    "\n",
    "## Penguin classification\n",
    "In this notebook, we will continue improving on the model we made to classify the penguin species. We will do this by using some of the transformers available to us within the scikit-learn API. We will cover the following aspects:\n",
    "\n",
    "1. Loading the data\n",
    "2. Preparing the data for sklearn\n",
    "3. Model creation & evaluation\n",
    "4. Data pre-processing\n",
    "5. Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading our data\n",
    "\n",
    "We load the data from the data folder, dropping the missing values on the way in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv('data/penguins.csv')\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing the data for scikit-learn\n",
    "\n",
    "Here we drop missing values & keep required fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = penguins.dropna()\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our knowledge of Pandas we create our feature matrix *X* and target vector *y*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "\n",
    "X = penguins.loc[:, feature_columns]\n",
    "y = penguins.loc[:, 'species']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data into train & test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model creation and evaluation\n",
    "\n",
    "Now we're ready to create our machine learning model! \n",
    "\n",
    "Scikit-learn has a rich collection of algorithms readily available. Depending on the case you are working on, scikit-learn most likely has a model that will suit your purposes. \n",
    "\n",
    "#### Scikit-Learn API usage steps when training a model\n",
    "1. Choosing a model class and importing that model \n",
    "2. Choosing the model hyperparameters by instantiating this class with desired values.\n",
    "3. Training the model to the preprocessed train data by calling the `fit()` method of the model instance.\n",
    "4. Evaluating model's performance using available metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: import the chosen algorithm \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Step 2: instantiate the model with the chosen hyperparameters\n",
    "model = DecisionTreeClassifier(max_depth=2)\n",
    "\n",
    "# Step 3: train the model with the training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now trained a model that can be used to make predictions on new data. Remember our test set? That's new, unseen data to the model that we can now create predictions on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Scikit-learn's implementations of the possible metrics readily available, such as accuracy. \n",
    "\n",
    "$\\text{accuracy} = \\frac{correct}{total}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good! \n",
    "\n",
    "But accuracy is not the only metric you could be interested in. Alternatives are, for example, _precision_ and _recall_. \n",
    "\n",
    "* _Precision_ is the proportion of positive identifications that was actually correct. \n",
    "* _Recall_ is the proportion of actual positives that was identified correctly.\n",
    "* _F1 score_ is a function of precision and recall, that you use when you seek a balance between precision and recall. \n",
    "\n",
    "Precision, recall and F1 are also all available with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_pred, y_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# Pre-processing the data\n",
    "\n",
    "A decision tree, the algorithm we used, works pretty well out of the box without requiring much pre-processing to the data, given that all the data is numeric. However, this is not the case for all machine learning algorithms. There are also some chunks of the data that we've omitted to allow for this model to be built.\n",
    "\n",
    "### Encoding of categorical variables\n",
    "\n",
    "We've initially omitted two possible features: _sex_ and _island_. Many of the popular machine learning algorithms require the data to be numeric. This will have to be taken care of before we can include these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins['sex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins['island'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex', 'island']\n",
    "\n",
    "X = penguins.loc[:, feature_columns]\n",
    "y = penguins.loc[:, 'species']\n",
    "\n",
    "print(f'The shape of feature matrix X is: {X.shape}')\n",
    "print(f'The shape of target vector y is: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['sex', 'island']\n",
    "X_train[categorical_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like the machine learning estimators, in Scikit-Learn, the preprocessing algorithms are implemented as Python objects. They are referred to as _transformers_.\n",
    "\n",
    "Once you have picked the transformer algorithm you will use, you instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers have a `.fit()` method implemented so that they can learn the paramters to transform data. They also have a `.transform()` method implemented that can apply the learned transformation to new data. A shortcut is `.fit_transform()`, which performs the transformation directly after the parameters have been learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.fit(X_train[categorical_columns])\n",
    "output = encoder.transform(X_train[['sex', 'island']])\n",
    "output[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = encoder.fit_transform(X_train[categorical_columns])\n",
    "output[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature matrix it would be more practical to encode all categorical features at once. One way of doing this could be to use the OrdinalEncoder, like we saw, which can ordinally encode multiple features at the same time.\n",
    "\n",
    "However, we do not want to encode all of the features, only those that are categorical. Rather than doing this manually, we can use `ColumnTransformer` to achieve exactly that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "    (\"ordinal\", OrdinalEncoder(handle_unknown='error'), categorical_columns)\n",
    "    ], remainder=\"passthrough\") \n",
    "\n",
    "# The output of fit_transform is no longer a pandas df, but now a numpy matrix. \n",
    "X_train_encoded = ct.fit_transform(X_train)\n",
    "\n",
    "print(X_train_encoded[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the `.transform()` method on the ColumnTransformer we can ensure the test set is encoded in the same way and the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_encoded = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a potential issue with ordinally encoding categorical features: it assumes an inherent order between the categories.\n",
    "\n",
    "Whilst this may be acceptable for some categorical features, e.g. medals in the Olympics, it does not make sense in the context of Penguin species. Indeed for a number of machine learning algorithms this can be detrimental.\n",
    "\n",
    "An alternative approach is to use one-hot or dummy encoding. This type of encoding can be obtained with the `OneHotEncoder`, which transforms each categorical feature with $n$ possible values into $n$ binary features, with one of them 1, and all others 0.\n",
    "\n",
    "*i.e. the species column would be transformed as follows*\n",
    "```\n",
    "Adelie     -> [1, 0, 0]\n",
    "Chinstrap  -> [0, 1, 0]\n",
    "Gentoo     -> [0, 0, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "    (\"onehot\", OneHotEncoder(drop=None, handle_unknown='error'), categorical_columns)\n",
    "    ], remainder=\"passthrough\") \n",
    "\n",
    "X_train_encoded = ct.fit_transform(X_train)\n",
    "X_test_encoded = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to encode a column $n$ possible values into $n$ - 1 columns, instead of $n$ columns, by using the drop parameter. his is useful to avoid co-linearity in the input matrix in some classifiers. \n",
    "\n",
    "*i.e. when dropping the first column, the species column would be transformed as follows*\n",
    "```\n",
    "Adelie     -> [0, 0]\n",
    "Chinstrap  -> [1, 0]\n",
    "Gentoo     -> [0, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "    (\"onehot\", OneHotEncoder(drop=\"first\", handle_unknown='error'), categorical_columns)\n",
    "    ], remainder=\"passthrough\") \n",
    "\n",
    "# The output of fit_transform is no longer a pandas df, but now a numpy matrix. \n",
    "X_train_encoded = ct.fit_transform(X_train)\n",
    "\n",
    "X_train_encoded = ct.fit_transform(X_train)\n",
    "X_test_encoded = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of features before:\", X_train.shape[1])\n",
    "print(\"number of features after:\", X_train_encoded.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to plug in our new, extended data with the two additional features into our decision tree classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train_encoded, y_train)\n",
    "y_pred = model.predict(X_test_encoded)\n",
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the score has improved! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling of the data\n",
    "\n",
    "Dealing with missing values and converting the categorical variables to numeric values was sufficient preprocessing for the decision tree. However, you might be incentivised to use another machine learning algorithm. \n",
    "\n",
    "\n",
    "Your preprocessed data may contain attributes with a mixtures of scales for various quantities such as dollars, kilograms and sales volume, or - such as in this case - measurements in grams and milimeters. Many machine learning algorithms perform better when numerical input variables are scaled to a standard range. This includes algorithms that use a weighted sum of the input, like linear regression, and algorithms that use distance measures, like k-nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "model.fit(X_train_encoded, y_train)\n",
    "y_pred = model.predict(X_test_encoded)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(f'Model accuracy: {model.score(X_test_encoded, y_test)}')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-Nearest Neighbors, for instance, does not seem to performed very well on the unscaled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `StandardScaler` is a transformer that transforms features by removing the mean and scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_train_scaled[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data has now been scaled and we can retry the k-nearest neighbors classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(f'Model accuracy: {model.score(X_test_scaled, y_test)}')\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A huge improvement! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines\n",
    "\n",
    "It was a bit cumbersome to manually make sure we use the transformers appropriately, on the right dataset, and run the classifier. In addition to scaling and encoding, you might have other preprocessing steps as well, such as imputing your missing values, creating polynomial features, and so on. \n",
    "\n",
    "\n",
    "Pipelines allow us to encapsulate all the preprocessing steps (imputing, scaling, encoding of variables and so on), as well as the final model, into a single Scikit-Learn estimator. \n",
    "\n",
    "> **\"If you aren’t using pipelines you’re probably doing [Scikit-Learn] wrong.\"** - [Andreas Muller, Core Developer of Scikit-learn ](https://towardsdatascience.com/want-to-truly-master-scikit-learn-2-essential-tips-from-the-official-developer-himself-dada6ff56b99)\n",
    "\n",
    "#### Why pipelines?\n",
    "\n",
    "Whilst the statement above was probably an exaggeration, they are a great way to keep your code clean, consistent and mistake-free. \n",
    "\n",
    "- Pipelines simplify and automate many steps in preprocessing and model training. \n",
    "- They give your workflow order and make it easier to read and understand. Later we will see how they can also be very useful during model optimization. \n",
    "- In addition to this including preprocessing as part of our model pipeline we can **avoid information leaks**\n",
    "\n",
    "To demonstrate this we will reperform the preprocessing techniques we tried on the Penguins dataset using a pipeline.\n",
    "\n",
    "We will first redefine the column transformer as specified above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "    (\"onehot\", OneHotEncoder(drop='first', handle_unknown='error'), categorical_columns)\n",
    "    ], remainder=\"passthrough\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Scikit-Learn's `Pipeline` we can create our own transformer that encodes the catgorical features and then scales the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "preprocessor = Pipeline(steps=[\n",
    "      ('encoding', ct),\n",
    "      ('scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "X_test_transformed[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline now acts as a transformer, such as the scaler and the encoder, that combines multiple steps. \n",
    "\n",
    "We can also include the model in a pipeline, such that preprocessing and classification can be performed in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_pipeline = Pipeline(steps=[\n",
    "    ('encoding', ct),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline can be trained, much like a regular model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred  = penguins_pipeline.predict(X_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can even use the pipeline object to make predictions on new data. Notice how we do not have to process our `X_test` manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Piplines are an incredibly useful tool to organise all your preprocessing and modelling steps into one simple object. This not only prevents data leakage and removes the need for manual executing of all the transformer steps on the right datasets, but also allows you to tune your hyperparameters as a combination of preprocessing steps & chosen algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Exercise ** \n",
    "\n",
    "Create a pipeline with the following things: \n",
    "* **Model**: choose a classifier.\n",
    "* **Scaling**: try a different scaler, e.g. [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)  or [RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html). When do you imagine you'd use these instead of the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)? \n",
    "* **Encoding**: encode the categorical variables.\n",
    "* **Impute missing values**: we will introduce some missing values into the data. Replace them with the [Simple Imputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html).\n",
    "\n",
    "Use `X_train_nan` to train your model and `X_test_nan` for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_missing_values(X_orig, p=0.05): \n",
    "    '''\n",
    "    Add missing values (NaN) randomly to the columns in the dataframe.\n",
    "    \n",
    "    X_orig: Pandas DataFrame. Feature matrix.\n",
    "    p: float. Percentage of values to set to NaN. \n",
    "    '''\n",
    "    X = X_orig.copy()\n",
    "    for col in X.columns: \n",
    "        values = np.array(X[col].tolist())\n",
    "        mask = np.random.rand(len(values)) > p\n",
    "        values[np.where(~mask)] = np.nan\n",
    "        X[col] = values\n",
    "    return X\n",
    "\n",
    "X_train_nan = add_missing_values(X_train) \n",
    "X_test_nan = add_missing_values(X_test)\n",
    "\n",
    "X_train_nan.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Scikit-learn is an excellent, resourceful tool for machine learning in Python. We've seen how we can split a dataset with `train_test_split` into a train and test set, create and train a model, use the trained model to create predictions, and how to use the tools from `sklearn.metrics` to evaluate how good the model is. We have also seen preprocessing techniques like scaling and encoding your categorical variables, and the use of pipelines. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
