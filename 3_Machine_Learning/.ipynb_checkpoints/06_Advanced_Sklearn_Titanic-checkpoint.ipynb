{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Advanced: Titanic dataset\n",
    "<img src=\"images/04_sklearn_advanced/titanic.png\" style=\"display: block;margin-left: auto;margin-right: auto;width: 300px\"/>\n",
    "\n",
    "## About the data\n",
    "In this notebook we will work with the [Titanic dataset](https://www.kaggle.com/c/titanic/data), containing historical data about Titanic passengers and whether they survived the wreck.\n",
    "\n",
    "\n",
    "| Variable | Definition                                  | Key                                            |\n",
    "| -------- | ------------------------------------------- | ---------------------------------------------- |\n",
    "|  |\n",
    "| survival | Survival                                    | 0 = No, 1 = Yes                                |\n",
    "| pclass   | Ticket class                                | 1 = 1st, 2 = 2nd, 3 = 3rd                      |\n",
    "| sex      | Sex                                         |                                                |\n",
    "| age      | Age in years                                |                                                |\n",
    "| sibsp    | \\# of siblings / spouses aboard the Titanic |                                                |\n",
    "| parch    | \\# of parents / children aboard the Titanic |                                                |\n",
    "| fare     | Passenger fare                              |                                                |\n",
    "| cabin    | Cabin number                                |                                                |\n",
    "| embarked | Port of Embarkation                         | C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "\n",
    "## Titanic disaster survivor classification\n",
    "In this notebook, we will demonstrate how to model whether a given passenger aboard the Titanic is expected to have survived the disaster with scikit-learn. \n",
    "\n",
    "Due to the nature of the data, this dataset requires a little more elaborate processing before we can get started. This allows us to introduce:\n",
    "* **Preprocessing** with scikit-learn transformers\n",
    "*  **Pipelines**\n",
    "* **Cross-validation** \n",
    "* **Grid-search** for hyperparameter tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading in the data\n",
    "\n",
    "The data is loaded from a package called _seaborn_. Seaborn was imported in the first cell (`import seaborn as sns`) and has a `.load_dataset` function that provides us with the Titanic dataset as a Pandas DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>Second</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>B</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>Q</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass     sex   age  sibsp  parch     fare embarked   class  \\\n",
       "0           0       3    male  22.0      1      0   7.2500        S   Third   \n",
       "1           1       1  female  38.0      1      0  71.2833        C   First   \n",
       "2           1       3  female  26.0      0      0   7.9250        S   Third   \n",
       "3           1       1  female  35.0      1      0  53.1000        S   First   \n",
       "4           0       3    male  35.0      0      0   8.0500        S   Third   \n",
       "..        ...     ...     ...   ...    ...    ...      ...      ...     ...   \n",
       "886         0       2    male  27.0      0      0  13.0000        S  Second   \n",
       "887         1       1  female  19.0      0      0  30.0000        S   First   \n",
       "888         0       3  female   NaN      1      2  23.4500        S   Third   \n",
       "889         1       1    male  26.0      0      0  30.0000        C   First   \n",
       "890         0       3    male  32.0      0      0   7.7500        Q   Third   \n",
       "\n",
       "       who  adult_male deck  embark_town alive  alone  \n",
       "0      man        True  NaN  Southampton    no  False  \n",
       "1    woman       False    C    Cherbourg   yes  False  \n",
       "2    woman       False  NaN  Southampton   yes   True  \n",
       "3    woman       False    C  Southampton   yes  False  \n",
       "4      man        True  NaN  Southampton    no   True  \n",
       "..     ...         ...  ...          ...   ...    ...  \n",
       "886    man        True  NaN  Southampton    no   True  \n",
       "887  woman       False    B  Southampton   yes   True  \n",
       "888  woman       False  NaN  Southampton    no  False  \n",
       "889    man        True    C    Cherbourg   yes   True  \n",
       "890    man        True  NaN   Queenstown    no   True  \n",
       "\n",
       "[891 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df = sns.load_dataset('titanic')\n",
    "titanic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "Take some time to examine the dataset. Below are some suggestions for what you may want to investigate. Note that this is not an exhaustive list; see if you can find something interesting!\n",
    "\n",
    "    - How much data do we have? how many features?\n",
    "    - What do the features represent\n",
    "    - What datatypes does it contain? Are there any missing values?\n",
    "    \n",
    "    - Investigate how many different values some of the categorical features contain\n",
    "    - Is there any redundant information?\n",
    "    \n",
    "    - Produce some summary statistics for the different features\n",
    "    - Are any of the features correlated?\n",
    "    - Group the data by the survived column and compare statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your EDA code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about it:\n",
    "- What do you imagine a machine learning algorithm will do with missing values? \n",
    "- Do you think duplicate columns will improve or hurt performance? \n",
    "- Do you imagine a machine learning algorithm can deal with categorical values (strings)? \n",
    "\n",
    "Clearly some issues need to be resolved before we can do machine learning on this dataset.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write down your thoughts here.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "\n",
    "Raw datasets are often not suitable for machine learning algorithms. For example, the dataset may contain categorical features or have missing values. Preprocessing the dataset to ensure that machine learning is feasible is therefore an import phase of a project.\n",
    "\n",
    "In the dataset we found plenty of interesting variables, though quite a few need preprocessing. Let's first remove the redundant columns and drop rows with missing values*.\n",
    "\n",
    "**With Scikit-Learn we can actually impute missing values but for now this approach will be sufficient*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 8) (712,)\n"
     ]
    }
   ],
   "source": [
    "# Remove irrelevant features & missing values.\n",
    "# Later we will discuss alternatives to this.\n",
    "titanic_processed = (\n",
    "    titanic_df\n",
    "    .drop(['embarked', 'sex', 'adult_male','deck','alive', 'class'], axis=1)\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Get the feature matrix & target vector\n",
    "titanic_features = (\n",
    "    titanic_processed\n",
    "    .drop(['survived'], axis=1)\n",
    ")\n",
    "titanic_labels = titanic_processed['survived']\n",
    "\n",
    "# Notice that we now use Pandas dataframe & series rather than numpy arrays - this is also possible with sklearn!\n",
    "print(titanic_features.shape, titanic_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>who</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>man</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>woman</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>woman</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>woman</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>man</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass   age  sibsp  parch     fare    who  embark_town  alone\n",
       "0       3  22.0      1      0   7.2500    man  Southampton  False\n",
       "1       1  38.0      1      0  71.2833  woman    Cherbourg  False\n",
       "2       3  26.0      0      0   7.9250  woman  Southampton   True\n",
       "3       1  35.0      1      0  53.1000  woman  Southampton  False\n",
       "4       3  35.0      0      0   8.0500    man  Southampton   True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    0\n",
       "Name: survived, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Variable encoding\n",
    "\n",
    "It would still be rather difficult to work with the raw dataset. One reason is that it contains categorical values that are represented as strings, e.g passenger class comes as text: \"First\", \"Second\" and \"Third\". A machine learning model cannot handle strings; it requires the input to be numeric. Therefore, we need to transform these categorical variables to numeric values that represent the same information.\n",
    "\n",
    "Fortunately, scikit-learn makes that easy for us to do.\n",
    "\n",
    "#### Encoding target vector y\n",
    "\n",
    "We can numerically encode a column using the `LabelEncoder` from Scikit-Learn.\n",
    "\n",
    "Normally this is used for the target or label column (although in our case this is not necessary). However, it can also be used for columns in the feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like the machine learning estimators, in Scikit-Learn, the preprocessing algorithms are implemented as Python objects. They are referred to as *transformers*.\n",
    "\n",
    "Once you have picked the transformer algorithm you will use, you instantiate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers have a `.fit()` method implemented so that they can learn the paramters to transform data. The `.transform()` method is then used to perform the transformation. Let's demonstrate this on the `embark_town` column of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transformation:\n",
      "0      Southampton\n",
      "1        Cherbourg\n",
      "2      Southampton\n",
      "3      Southampton\n",
      "4      Southampton\n",
      "          ...     \n",
      "885     Queenstown\n",
      "886    Southampton\n",
      "887    Southampton\n",
      "889      Cherbourg\n",
      "890     Queenstown\n",
      "Name: embark_town, Length: 712, dtype: object\n",
      "\n",
      "\n",
      "After transformation:\n",
      "[2 0 2 2 2 2 2 2 0 2 2 2 2 2 2 1 2 2 2 1 2 2 2 2 0 2 0 2 2 2 0 2 2 0 1 2 2\n",
      " 2 0 2 0 2 0 2 2 0 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0\n",
      " 0 2 2 2 2 2 2 2 2 2 2 0 2 2 0 2 1 2 0 2 2 0 2 2 0 2 2 0 2 2 2 2 0 2 2 2 0\n",
      " 2 2 1 2 2 2 2 2 2 2 2 2 2 0 1 2 2 2 2 2 2 2 2 2 2 1 2 2 0 2 0 2 2 2 2 2 2\n",
      " 1 2 2 2 2 2 0 0 2 2 2 2 0 2 2 2 0 1 0 2 2 2 2 0 2 2 0 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 0 1 2 2 2 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 2 2 2 2 0 2 2\n",
      " 1 2 1 2 2 2 0 2 2 2 1 2 0 0 2 2 0 2 0 2 2 0 0 0 0 0 2 2 2 2 2 2 2 0 2 2 1\n",
      " 2 0 2 2 2 0 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 0 0 2 2 0 0 0 2\n",
      " 2 0 2 2 0 0 2 0 0 2 2 2 2 2 0 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2\n",
      " 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 0 0 0 2 2 2 2 2\n",
      " 2 2 2 0 2 2 0 2 2 2 2 2 2 2 2 0 2 0 2 2 2 2 0 2 0 2 2 2 1 2 2 0 2 2 2 1 2\n",
      " 0 2 2 2 2 2 2 2 0 1 2 2 2 2 0 2 2 2 0 0 2 2 2 2 0 2 2 2 2 0 2 0 2 2 0 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 2 2 0 2 2 0 2 2 2 2 0 2 2 0 2 2 2 0 2\n",
      " 2 2 2 2 2 2 2 0 2 0 2 2 2 1 2 2 2 2 0 2 2 2 2 2 2 0 2 0 0 2 0 2 2 2 1 2 1\n",
      " 2 0 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 2 2 2 0 2 2 2 2 2 0 0 2 2 2 0 2 0\n",
      " 2 0 1 2 2 2 2 2 0 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 0 2 0 2 2 2\n",
      " 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 1 2 2 2 2 2 2 2 2 0 2 2 2 2 2 1 2 0\n",
      " 2 2 2 2 2 0 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 0 0 2 2 2 0\n",
      " 2 2 2 2 0 0 2 2 0 2 2 2 0 2 2 2 2 2 0 2 2 2 2 2 0 2 2 2 2 2 2 0 0 2 2 0 2\n",
      " 2 2 2 2 1 2 2 0 1]\n"
     ]
    }
   ],
   "source": [
    "print('Before transformation:')\n",
    "print(titanic_features['embark_town'])\n",
    "\n",
    "label_encoder.fit(titanic_features['embark_town'])\n",
    "new_col = label_encoder.transform(titanic_features['embark_town'])\n",
    "\n",
    "print('\\n\\nAfter transformation:')\n",
    "print(new_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They also have a `.fit_transform()` method implemented so that the transformation can be performed directly after the paramters have been learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before transformation:')\n",
    "print(titanic_features['alone'])\n",
    "\n",
    "new_col = label_encoder.fit_transform(titanic_features['alone'])\n",
    "\n",
    "print('\\n\\nAfter transformation:')\n",
    "print(new_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Ordinally encoding feature matrix X\n",
    "\n",
    "The `LabelEncoder` is a transformer that is specifically meant to convert the target variable. It can therefore only except a vector and is not able to encode multiple columns at once. \n",
    "\n",
    "In our feature matrix, however, we also have categorical columns that we need to transform, and it would be more practical to encode them all at once (rather than one column at the time). \n",
    "\n",
    "One way of doing this could be to use the `OrdinalEncoder`, which can ordinally encode multiple features at the same time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "transformed_data = ordinal_encoder.fit_transform(titanic_features[[ 'who', 'embark_town', 'alone']])\n",
    "transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice two things here:\n",
    "1. Our output is no longer a Pandas DataFrame -- it's a `numpy` array! \n",
    "2. We only have 3 features left -- all the features that weren't transformed were discarded! \n",
    "\n",
    "The first point is something we simply have to deal with. We could transform our data back to a Pandas DataFrame to make the data more readable to us, but scikit-learn is quicker with `numpy` arrays than Pandas DataFrames, so we could also leave it as is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(transformed_data, columns=[ 'who', 'embark_town', 'alone'])\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second point, however, is more important. We've lost valuable information. \n",
    "\n",
    "A solution to this would be to use the `ColumnTransformer`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "\n",
    "categorical_columns = [ 'who', 'embark_town', 'alone']\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "    (\"ordinal\", OrdinalEncoder(), categorical_columns)\n",
    "    ], remainder=\"passthrough\") \n",
    "\n",
    "# The output of fit_transform is no longer a pandas df, but now a numpy matrix. \n",
    "feature_matrix = ct.fit_transform(titanic_features)\n",
    "\n",
    "print(feature_matrix[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the ColumnTransformer will affect the order of your columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 One-hot encoding features\n",
    "\n",
    "What issues could we get from using an `OrdinalEncoder()` on some of the categorical values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way of encoding categorial values may work for problems where there is a natural ordinal relationship between the categories, and in turn the integer values, such as labels for temperature ‘cold’, warm’, and ‘hot’.\n",
    "\n",
    "There may be problems when there is no ordinal relationship and allowing the representation to lean on any such relationship might be damaging to learning to solve the problem. An example might be the labels ‘red’, 'blue' and ‘green’\n",
    "\n",
    "In these cases, we would like to give the algorithm more expressive power, we require one-hot encoding.\n",
    "\n",
    "*Ordinal encoding*\n",
    "```\n",
    "Southampton    -> 0\n",
    "Cherbourg      -> 1\n",
    "Queenstown     -> 2\n",
    "```\n",
    "\n",
    "*One-hot encoding*\n",
    "```\n",
    "Southampton    -> [1, 0, 0]\n",
    "Cherbourg      -> [0, 1, 0]\n",
    "Queenstown     -> [0, 0, 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we can use sklearn's `OneHotEncoder()` for this purpose. This can also help us encode multiple features at the same time.\n",
    "\n",
    "Again, we do not want to encode *all* of the features. We only want to pass some of the columns into the `OneHotEncoder()`. Like before, we use the `ColumnTransformer()` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(titanic_features.shape)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "    (\"onehot\", OneHotEncoder(drop='first'), [ 'who', 'embark_town', 'alone'])\n",
    "    ], remainder=\"passthrough\") \n",
    "\n",
    "# The output of fit_transform is no longer a pandas df, but now a numpy matrix. \n",
    "feature_matrix = ct.fit_transform(titanic_features)\n",
    "\n",
    "print(feature_matrix[0:5])\n",
    "\n",
    "print(feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keep in mind:**\n",
    "\n",
    "We use the `drop='first'` option in OneHotEncoder, which ensures that we keep *n-1* dummies per categorical variable.\n",
    "\n",
    "*One-hot encoding with drop='first'*\n",
    "```\n",
    "Southampton    -> [0, 0]\n",
    "Cherbourg      -> [1, 0]\n",
    "Queenstown     -> [0, 1]\n",
    "```\n",
    "\n",
    "`ColumnTransformer` can also simultaneously apply a different transformation to other columns if we specify an extra step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature Scaling\n",
    "\n",
    "Your preprocessed data may contain attributes with a mixtures of scales for various quantities such as dollars, kilograms and sales volume. \n",
    "\n",
    "However, many machine learning algorithms perform better when numerical input variables are scaled to a standard range. [For example](https://gdd.li/04_sklearn), algorithms that use distance measures, like k-nearest neighbors.\n",
    "\n",
    "We want to ensure our variables share a similar scale. However, it is important to be aware of when to do feature scaling to avoid data leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Leakage\n",
    "\n",
    "We want to avoid any information from the test set leaking into the training set; it is important our machine learning algorithm learns only from the training data! \n",
    "\n",
    "If information from the test set *does* leak into the training data, it may cause our metrics to overestimate our model's performance.\n",
    "\n",
    "Therefore we must do the train-test split *before* we do feature scaling (and other forms of preprocessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename to X and y for consistency. \n",
    "X = feature_matrix\n",
    "y = titanic_labels\n",
    "\n",
    "print(type(X))\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=111, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing classification, we can set `stratify` to ensure a consistent split across training and test sets. This can be useful to get a more reliable score for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature scaling with sklearn\n",
    "\n",
    "`StandardScaler()` and `MinMaxScaler()` from `sklearn.preprocessing` allow us to scale the data together with applying the `fit_transform()` method\n",
    "\n",
    "Remember to always first separate the data into train/test (and validation) sets and only then use feature scaling. If you take the mean and variance of the whole dataset you'll be introducing future information into the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `StandardScaler()` transforms features by removing the mean and scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After instantiating the transformer, we learn *how* to transform the dataset on the training set with `.fit()`\n",
    "\n",
    "We then transfom the train set with `.transform()`.\n",
    "\n",
    "These two steps can be performed at once with `.fit_transform()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.std(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.min(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.max(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the `.transform()` on the test set,  we ensure it is encoded in the same way as the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MinMaxScaler()` transforms features to be with a certain range, e.g. 0 & 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.min(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.max(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other scaling options to use too. For example, the `RobustScaler`.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "1. Perform scaling on the train & test set with the RobustScaler. \n",
    "2. How do the MinMaxScaler, StandardScaler and RobustScaler differ? Can you think of circumstances where you'd use one over the other? \n",
    "3. Would you scale _all_ columns with the same scaler, or would you treat columns differently (e.g. one column with MinMaxScaler and another with the StandardScaler). Why or why not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "help(RobustScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your RobustScaler code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Other Preprocessing Tools in scikit-learn\n",
    "\n",
    "Further preprocessing tools from `sklearn.preprocessing` could also be applied to our dataset. For example,\n",
    "- Binarization: `Binarizer()`\n",
    "\n",
    "    A common operation on text count data where the analyst can decide to only consider the presence or absence of a feature rather than a quantified number of occurrences for instance.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "X = [[ 1., -1.,  2.],\n",
    "     [ 2.,  0.,  0.],\n",
    "     [ 0.,  1., -1.]]\n",
    "\n",
    "transformer = Binarizer(threshold = 0) \n",
    "transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Imputing Missing Values: `Imputer()`\n",
    "\n",
    "    Address missing values in a column by imputting the mean, median, mode or another constant value.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X = [[1, 2, 3], \n",
    "     [4, np.nan, 6], \n",
    "     [10, 5, 9]]\n",
    "\n",
    "transformer = SimpleImputer()\n",
    "transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [[np.nan, np.nan, np.nan]]\n",
    "\n",
    "transformer.transform(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Generating Polynomial Features: `PolynomialFeatures()`\n",
    "\n",
    "    Generate a new feature matrix consisting of all polynomial combinations of the features. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2]. This can help the model to learn more complex relationships.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = [[0, 1],\n",
    "     [2, 3],\n",
    "     [4, 5]]\n",
    "\n",
    "\n",
    "transformer = PolynomialFeatures(interaction_only=True)\n",
    "transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.4 Scikit-Learn transformers\n",
    "\n",
    "The diagrams below are visualisations of a Scikit-Learn transformer.\n",
    "\n",
    "`.fit()`\n",
    "\n",
    "<img src=\"images/04_sklearn_advanced/transform.png\" style=\"display: block;margin-left: auto;margin-right: auto;width: 600px\"/>\n",
    "\n",
    "`.fit_transform()`\n",
    "\n",
    "<img src=\"images/04_sklearn_advanced/fit_transform.png\" style=\"display: block;margin-left: auto;margin-right: auto;width: 600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipelines\n",
    "> **\"If you aren’t using pipelines you’re probably doing [Scikit-Learn] wrong.\"** - [Andreas Muller, Core Developer of Scikit-learn ](https://towardsdatascience.com/want-to-truly-master-scikit-learn-2-essential-tips-from-the-official-developer-himself-dada6ff56b99)\n",
    "\n",
    "<img src=\"images/04_sklearn_advanced/sklearn-pipe.png\" style=\"display: block;margin-left: auto;margin-right: auto;width: 600px\"/>\n",
    "\n",
    "#### What is a pipeline?\n",
    "\n",
    "Whilst the statement above was probably an exaggeration, they are a great way to keep your code clean, consistent and mistake-free. \n",
    "\n",
    "Pipelines encapsulate all the preprocessing steps (feature selections, scaling, encoding of variables and so on), as well as the final model, into a single Scikit-Learn estimator. \n",
    "\n",
    "- Pipelines simplify and automate many steps in preprocessing and model training. \n",
    "- They give your workflow order and make it easier to read and understand. Later we will see how they can also be very useful during model optimization. \n",
    "- In addition to this including preprocessing as part of our model pipeline we can **avoid information leaks**\n",
    "\n",
    "#### Pipeline for preprocessing\n",
    "\n",
    "An sklearn `Pipeline` simply requires us to specify a number of steps and what should happen at each of them. In our case we are going to add interaction terms for our features using `PolynomialFeatures()` and then scale the resulting data with `MinMaxScaler()` (we are not going to use `X_train_scaled` from the previous step).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "preprocess_pipeline = Pipeline(steps=[\n",
    "    ('PFeatures', PolynomialFeatures(interaction_only=True)),\n",
    "    ('scaler', MinMaxScaler())])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline for preprocessing and modelling! \n",
    "\n",
    "We can further extend this pipeline by adding the last step containing our machine learning model.\n",
    "\n",
    "We shall demonstrate this by adding an ML model to our pipeline (an SVM classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "titanic_pipeline = Pipeline(steps=[\n",
    "    ('PFeatures', PolynomialFeatures(2)),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('model', SVC(kernel='linear'))])\n",
    "\n",
    "titanic_pipeline.fit(X_train, y_train)\n",
    "y_pred = titanic_pipeline.predict(X_test)\n",
    "print(\"accuracy: \", round(accuracy_score(y_test, y_pred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beauty of sklearn pipelines is in the fact that it will also learn how to scale the data based on the `X_train` features and then automatically use it when we get predictions from `X_test`. So in practice it also removes some preprocessing efforts and unnecessary risks away from us.\n",
    "\n",
    "The diagram below is a visualisation of Scikit-Learn Pipeline\n",
    "\n",
    "<img src=\"images/04_sklearn_advanced/pipe-transform.png\" style=\"display: block;margin-left: auto;margin-right: auto;width: 600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a pipeline has been specified, model training is not much different from training a regular model. \n",
    "\n",
    "This also means we can easily evaluate whether certain preprocessing settings help our model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Validation\n",
    "\n",
    "When training our models, we should reserve a separate set of data for comparing model performance and picking hyperparameters, i.e. Should I use order 2 polynomial features or order 3?\n",
    "\n",
    "The test set should only be used to give a final evaluation of the model.\n",
    "\n",
    "\n",
    " <img src=\"images/04_sklearn_advanced/validation.png\" style=\"display: block;margin-left: auto;margin-right: auto;height: 100px\"/>\n",
    " \n",
    "Otherwise information from the test set leaks into our training process invalidating our metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Cross-Validation\n",
    "\n",
    "If we only have a small dataset, we may not have enough data to create a seperate training set. In addition to this, our validation results will still be biased to that split of the train/validation set. Cross-validation is used to address these issues.\n",
    "\n",
    " <img src=\"images/04_sklearn_advanced/crossvalidation.png\" style=\"display: block;margin-left: auto;margin-right: auto;height: 300px\"/>\n",
    "\n",
    "K-fold cross validation splits can be acquired using `KFold()` from `sklearn.model_selection`. Then, for each split, the model can be fit and relevant metrics can be computed.\n",
    "\n",
    "It may be however more practical to use `cross_val_score()` from `sklearn.model_selection`. It returns an array of cross-validated scores for a model/pipeline on the data.\n",
    "\n",
    "Let's see how our current model performs on the different validation splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(titanic_pipeline,\n",
    "                         X_train,\n",
    "                         y_train,\n",
    "                         cv = 5,\n",
    "                         scoring = 'accuracy'\n",
    "                        )\n",
    "\n",
    "print(f'Accuracy per fold: {scores.round(3)}')\n",
    "print(f'Accuracy mean over folds: {scores.mean().round(3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Think about it: \n",
    "1. What would be the advantage of a _higher_ number of folds for your cross-validation?\n",
    "    - What happens if the number of folds = 1? \n",
    "    - What happens if the number of folds = the number of data points in your train set (ie. 569 for the Titanic dataset)? \n",
    "    \n",
    "2. What would be the disadvantage of a higher number of folds? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write down your thoughts here... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Parameter tuning\n",
    "\n",
    "To find the best parameters, we often want to check how each combination of parameters performs. This process in known as a grid seach.\n",
    "\n",
    "We could manually collect the cross validation scores for each paramter combination. However, in Scikit-Learn this can be implemented directly.\n",
    "\n",
    "`GridSearchCV()` from `sklearn.model_selection` allows tuning of model's parameters using cross-validated grid search. \n",
    "\n",
    "It requires a dictionary of parameters and the options it should compare. This does not only have to be model parameters, but parts of preprocessing too. We simply need to reference the correct pipeline step using 'step_name__parameter_name' as one of the dictionary keys (note the double underscore between the step and parameter name). In the end, the best set of parameters will be selected based on the provided metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to the way the params dictionary is written:\n",
    "- A key is a string of the name of the model + 2 underscores + the specific parameter to search, for example classifier__criterion\n",
    "- A value is a list/tuple of all the parameters values you want to search\n",
    "\n",
    "This allows us to search possible parameters for any of the steps in the initial Pipeline (e.g. preprocessing too!) \n",
    "\n",
    "Note that with large datasets, many parameters and many cross validation folds, `GridSearchCV()` may take considerable time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#providing the list of parameters\n",
    "parameters_svm = {'PFeatures__degree': (1,2),\n",
    "                 'scaler': [MinMaxScaler(),StandardScaler(),RobustScaler() ]}\n",
    "\n",
    "#running grid search \n",
    "clf_svm = GridSearchCV(titanic_pipeline, parameters_svm, cv=5, scoring='accuracy')\n",
    "clf_svm = clf_svm.fit(X_train, y_train)\n",
    "\n",
    "print(f'CV accuracy score of the best SVM is: {clf_svm.best_score_:.3f}')\n",
    "print(f'Best parameters were: {clf_svm.best_params_}')\n",
    "#parameter importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to being easier to implement than a manual grid search cross-validation, `GridSearchCV()` helps us to avoid __information leaks__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results\n",
    "\n",
    "Once we've established the best parameters, we should use them to retrain on the *WHOLE* training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = clf_svm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy on the test set: {best_model.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is however usually a good idea to not just blindly take the \"best\" model, but manually compare different models from the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(clf_svm.cv_results_)\n",
    "cv_results.sort_values('rank_test_score').head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have finished, it will likely be helpful to save you model for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import dump, load\n",
    "# # Save the model\n",
    "# dump(best_model, 'best_model.joblib') \n",
    "\n",
    "# Load a saved model\n",
    "#best_model = load('best_model.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Scikit-Learn can be a very powerful tool to deal with machine learning problems including data splitting, preprocessing, model training and model selection. Its simple interface and detailed documentation allow it to be used even by users with little experience.\n",
    "\n",
    "* A huge variety of implemented models with sensible defaults, such as KMeans, SVC or RandomForest\n",
    "* Supports both pandas and numpy as data inputs\n",
    "* Data preprocessing techniques such as Scaler, OneHotEncoding, etc. \n",
    "* Validation tools such as train_test_split or cross validation\n",
    "* Pipelines to evaluate not only your model and its hyperparameters, but also your preprocessing steps.\n",
    "* GridSearch for parameter tuning selection\n",
    "* Consistent implementation and API which makes it easy to extend with your own implemented building blocks (scorers, preprocessing techniques and even models)\n",
    "\n",
    "Caveat: scikit-learn lacks tools to work with deep learning. For this purpose you would rather use deep learning libraries such as PyTorch or Tensorflow/Keras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
