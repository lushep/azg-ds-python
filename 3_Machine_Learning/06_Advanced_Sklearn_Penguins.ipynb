{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines Assignment: Palmer Penguins continued\n",
    "\n",
    "![](https://github.com/allisonhorst/palmerpenguins/raw/master/man/figures/lter_penguins.png) \n",
    "\n",
    "## Penguin classification\n",
    "In this notebook, we will continue improving on the model we made to classify the penguin species. We will do this by using some of the transformers available to us within the scikit-learn API. We will cover the following aspects:\n",
    "\n",
    "1. Loading the data\n",
    "2. Preparing the data for sklearn\n",
    "3. Model creation & evaluation\n",
    "4. Data pre-processing\n",
    "5. Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading our data\n",
    "\n",
    "We load the data from the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv('data/penguins_messy.csv')\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis\n",
    "\n",
    "As you can see, the data is slightly different than the previous file we saw! There seem to be some missing values, and we have a new column. \n",
    "\n",
    "Take some time to examine the dataset. Below are some suggestions for what you may want to investigate. Note that this is not an exhaustive list; see if you can find something interesting!\n",
    "\n",
    "    - How much data do we have? how many features?\n",
    "    - What do the features represent\n",
    "    - What datatypes does it contain? Are there any missing values?\n",
    "    \n",
    "    - Investigate how many different values some of the categorical features contain\n",
    "    - Is there any redundant information? Are you sure this information is redundant, ie. is there no information available in it that is not available in another column (e.g. a value that's NOT missing while it is in the original column) \n",
    "    \n",
    "    - Produce some summary statistics for the different features\n",
    "    - Are any of the features correlated?\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your EDA code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## 3. Pre-processing the data\n",
    "\n",
    "A decision tree, the algorithm we used, works pretty well out of the box without requiring much pre-processing to the data, given that all the data is numeric. However, this is not the case for all machine learning algorithms. There are also some chunks of the data that we've omitted to allow for the simple model to be built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'sex', 'island']\n",
    "\n",
    "X = penguins.loc[:, feature_columns]\n",
    "y = penguins.loc[:, 'species']\n",
    "\n",
    "print(f'The shape of feature matrix X is: {X.shape}')\n",
    "print(f'The shape of target vector y is: {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "transformer = OneHotEncoder()\n",
    "transformer.fit_transform(X_train[['sex', 'island']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT \n",
    "\n",
    "\n",
    "#### 1. Use the one-hot encoder to encode the categorical columns in your feature matrix. \n",
    "What columns would you want to one-hot encode? Do you use `drop='first'`? Are your results what you expect? Any reason why they might be different? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry if you don't get the right results immediately, move on to question 2! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Deal with your missing values first!\n",
    "Think about it: would you replace your missing values _before_ or _after_ encoding the data? \n",
    "\n",
    "Impute your missing values with the `SimpleImputer` from `sklearn.impute`, then encode the data. Verify that your results are what you expect. How many columns do you now have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Use the one-hot encoder in combination with the column transformer to encode your entire feature matrix. \n",
    "How many columns do you have now? Is that the number that you expect? Don't forget to use your imputed data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Moving to pipelines\n",
    "Create a pipeline that does two things:\n",
    "- Impute missing values\n",
    "- One hot encode your data\n",
    "\n",
    "Are your results equal to the results in step 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Expand your pipeline: model! \n",
    "You've dealt with your missing values, you've encoded your data.. you're ready for modelling! \n",
    "\n",
    "Expand your pipeline with two models: \n",
    "- DecisionTreeClassifier: `sklearn.tree.DecisionTreeClassifier`\n",
    "- KNeighborsClassifier: `sklearn.neighbors.KNeighborsClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your decision tree code here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your k-nearest neighbors code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Expand your pipeline: scaling! \n",
    "Does your data need to be scaled? If so, what scaler would you use? Add scaling to your pipeline with the k-Nearest Neighbors classifier. Do your results improve? \n",
    "\n",
    "**BONUS**: Do the same to the pipeline with the decision tree classifier. Do your results improve as much? Why do you think that is? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Tune your hyperparameters\n",
    "Use GridSearchCV in combination with your pipeline to search over: \n",
    "- at least  one model parameter (e.g. value for k) \n",
    "- at least one preprocessing parameter (e.g. imputer strategy, type of scaler, etc.) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS\n",
    "\n",
    "Get the best results possible! Add other preprocessing techniques if you like (e.g. polynomial features), try different models (maybe an SVM?), search over those hyperparameters... get the best result you can get! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Scikit-learn is an excellent, resourceful tool for machine learning in Python. We've seen how we can split a dataset with `train_test_split` into a train and test set, create and train a model, use the trained model to create predictions, and how to use the tools from `sklearn.metrics` to evaluate how good the model is. We have also seen preprocessing techniques like scaling and encoding your categorical variables, and the use of pipelines. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
