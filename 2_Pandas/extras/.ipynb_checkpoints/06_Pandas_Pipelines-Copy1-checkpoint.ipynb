{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern Pipelines\n",
    "\n",
    "In the notebooks so far, we have focussed on what pandas has to offer. However, we have not discussed how to use pandas in practice. In this notebook we will highlight a problem with certain workflows and demonstrate a workflow that could be adopted instead. \n",
    "\n",
    "Let's pretend that we've read in a timeseries and that this is the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ts_df():\n",
    "    dates = [str(_) for _ in pd.date_range(\"2018-01-01\", \"2019-01-01\")]\n",
    "    values = [np.nan if np.random.random() < 0.05 else _ for _ in np.random.normal(0, 1, 366)]\n",
    "    return pd.DataFrame({\"date\": dates, \"value\": values})\n",
    "\n",
    "date_df = make_ts_df()\n",
    "date_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start analysing the data, let's imagine we want to do the following:\n",
    "\n",
    "- Get rid of the redundant hours.\n",
    "- Clean the `nan` values.\n",
    "- Remove outliers. \n",
    "\n",
    "One way of doing it could be like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    date_df\n",
    "    .assign(date=lambda d: pd.to_datetime(d.date))\n",
    "    .dropna()\n",
    "    .loc[lambda d: d.value > -2.0]\n",
    "    .loc[lambda d: d.value < 2.0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the way we've been doing it so far, but we can do better.\n",
    "\n",
    "If you were to just look at the code above it could be a bit hard to understand what is going on.\n",
    "\n",
    "Also, if we were to get a new date dataframe, we'd have to start all over again. \n",
    "\n",
    "Whilst this is not a big issue when we are only doing 3 processing steps, as the amount of processing increases it could become time consuming.\n",
    "\n",
    "## Pipeline abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_types(dataf):\n",
    "    \"\"\"Remove the hours from dates\"\"\"\n",
    "    return (dataf\n",
    "            .assign(date=lambda d: pd.to_datetime(d['date'])))\n",
    "\n",
    "def clean_nan(dataf):\n",
    "    \"\"\"Get rid of rows with missing values\"\"\"\n",
    "    return (dataf.dropna())\n",
    "\n",
    "def fill_nan(dataf):\n",
    "    \"\"\"Fill nan values with 0\"\"\"\n",
    "    return (dataf.fillna(1))\n",
    "\n",
    "def remove_outliers(dataf):\n",
    "    \"\"\"Remove values greater than 2 and less than -2\"\"\"\n",
    "    return (dataf\n",
    "            .loc[lambda d: d['value'] > -2.0]\n",
    "            .loc[lambda d: d['value'] < 2.0])\n",
    "\n",
    "prep_df = (date_df\n",
    "           .pipe(parse_types)\n",
    "           .pipe(clean_nan)\n",
    "           .pipe(remove_outliers))\n",
    "prep_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.pipe()` method allows us to pass a function that accepts a dataframe as it's first argument. This is a very nice flow. \n",
    "\n",
    "- We can easily use this pipeline (or parts of this pipeline) for different datasets.\n",
    "\n",
    "<img src=\"../images/lego.jpg\" width=\"400\" height=\"400\" align=\"center\"/>\n",
    "\n",
    "- If there is ever a bug this pipeline will make it easier for us to figure out where it is. Since every step is merely a function, we'll know eactly where the process is breaking. \n",
    "\n",
    "- We can give the function a descriptive name and on a pipeline level this allows us to see \"what\" is happening \"when\". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. You may not have seen how the parse_types function works yet\n",
    "? parse_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveats \n",
    "\n",
    "We should be careful when we are writing `.pipe`-lines. The function going into a `.pipe()` might not be stateless. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(dataf):\n",
    "    dataf.columns = [\"a\", \"b\"]\n",
    "    return dataf \n",
    "\n",
    "date_df = make_ts_df()\n",
    "date_df.pipe(rename_columns).columns, date_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In such a situation it is best to include a `.copy()` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(dataf):\n",
    "    dataf = dataf.copy()\n",
    "    dataf.columns = [\"a\", \"b\"]\n",
    "    return dataf \n",
    "\n",
    "date_df = make_ts_df()\n",
    "date_df.pipe(rename_columns).columns, date_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful with this. We want our functions to be stateless, otherwise we lose the benefits.\n",
    "\n",
    "Alternatively, you could use the `.rename()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(dataf):\n",
    "    return dataf.rename(columns={\"date\": \"a\", \"value\": \"b\"})\n",
    "\n",
    "date_df = make_ts_df()\n",
    "date_df.pipe(rename_columns).columns, date_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline abstraction on higher Levels\n",
    "\n",
    "To fully appreciate what the pandas pipelines can do, let us rewrite one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(dataf, min_value=-2.0, max_value=2.0):\n",
    "    return (dataf\n",
    "            .loc[lambda d: d['value'] > min_value]\n",
    "            .loc[lambda d: d['value'] < max_value])\n",
    "\n",
    "prep_df = (date_df\n",
    "           .pipe(parse_types)\n",
    "           .pipe(clean_nan)\n",
    "           .pipe(remove_outliers, max_value=0.5))\n",
    "prep_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.pipe()` can accept keyword arguments. This allows you to change, say, threshold values on a high level. No need to change the original function, you can change things from a higher level. This is great because it will encourage you to write functions that are general. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "> **\"Pipelines are the only correct way to write pandas.\"**\n",
    "\n",
    "This is a bold statement, but some of people very strongly about this. \n",
    "\n",
    "Even if you take this statement with a grain of salt, it is important to write your code in such a way that your notebooks remains clear - if it takes a lot of effort to understand the code of your colleagues, then your team will be slower than you want it to be. \n",
    "\n",
    "A notebook is a great scratchpad, but that is no excuse to write unclear code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
